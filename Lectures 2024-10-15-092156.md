---
tags:
  - status/pending
course:
---





# Concetto

- Underfit e overfit
	- Underfit e troppo approssimativo
	- Overfit interpola anche il rumore dei dati reali
- Vandermonde crea la matrice associata
- Regolarizzazione
	- r(\alpha) e l'errore dei calcoli, il residuo
		- non deve essere 0 ma neanche troppo grande, overfit e underfit
			- Underfit evitato se arriviamo all'arresto dell L curva
			- Modello AI non aundergitta perche facile
	- Per l'overfit
		- Sparo un valore piu alto cosi overfitto a forza poi risolvo
		- Minimo quadrato e aggiunto un pezzo
		- Se togliessimo il minimo quadrato significherebbe mettere a \alpha= 0
		- Fino a quando sei soddisfatto vai di minimo quadrato e quando sei a posto manda tutto a 0 col secondo pezzo, detto termine di regolarizzazione
			- Lambda iperparametro se lambda e grande la soluzione sara come zero se lambda e grande a e a 0 se e buono lambda la soluzione e buona 
			- Sotto e gradiente di quest'ultima
			- Applichiamo cholesky a tutta la formula trovata(il gradiente)
		- Per scegliere il labda ottimale prendo una lista di lambda e seleziono quella che fitta meglio
		- Scegliamo d $\geq$ 5 perche volgiamo evitare di underfittare
		- Risultati
			- lambda 0 o molto piccola vibriamo molto, non reglolarizziamo e vince il minimo quadrato, quindi, grande rimore
			- lambda molto grande la curva si schiaccia, ci underfitta perche lambda domina il minimo quadrato
		- Adesso devo scegliere lambda ma cosa cambia da scegliere d
			- Lambda essendo reale ho piu scelta
			- Reti neurali moderne utilizzano lambda
	- Metodo LASSO
		- Perche proprio la norma ^2
			- Perche la derivata non capita sugli assi, quindi, i valori tendono a non essere zero, invece, la norma non alla seconda la tangente e sempre sugli assi
		- Nel metodo LASSO la norma di $\alpha$ e norma 1
	- Ottimizzazione
		- Dobbiamo minimizzare la distanza totale dei nodi
		- Trovare il percorso ottimale
		- Insieme di vincoli
			- Se $\Omega = \mathbb{R}^n$ il problema e svincolato
			- Altrimenti e vincolato se sottoinsieme stretto
			- Problemi discreti se utilizzano numeri interi 



- LEX
	- Generatore di scanner, specifico di Unix
	- FLEX piu recente
	- Input
		- File.l che ha insieme di definizioni e assiomi
	- Output
		- Programma in C che genera un automa in grado di riconoscere il linguaggio
	- Lo siamo poi in pasto ad un compulatore C che genera un eseguibile
	- Diamo in input ad esso un programma scritto in l e abbiamo una lista di tokens in output
	- File.l
		- File lex.yy.c
			- Implementa il DFA riconoscitore del linguaggio
				- Produce il token relativo al lessema in input, se riconoscibile ed esegue l'azione relativa
				- Ricerca sempre il pattern+lungo se uno e prefisso dell'altro
				- Se uno e caso speciale dell'altro sceglie il primo in elenco
					- Applicare queste regole nella simulazione del DFA relativo all'NFA
				- Le parole riservate devono essere elencate prima come parole riservate prima di tutto questo
	- A guidare questo processo e l'analizzatore sintattico che lavora dando instruzioni all;aanalizzatotore lessicale

- Proprieta algoritmiche dei linguaggi regolari
	- Proprieta che sono utili per dimostrare che esistono linguaggi a non essere regolari
	- Regolare se
		- Le produzioni hanno un terminale seguito da un non-terminale, o solo un terminale
	- Un linguaggio libero non ha un AUtoma associato perche avrebbe stati un numero di stati non finiti
- Pumping lemma
	- L linguaggio regolare come ipotesi
	- Chiede all'orale la dimostazione relativa
	- Se il linguaggio e regolare vale il lemma pumping
		- Se non vale il lemma allora il linguaggio non e regolare
	- Contronominale del pumping lemma ancora + importante e utilizzata 




# Riferimenti
[[Minimizzazione DFA]]