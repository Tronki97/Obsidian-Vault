---
tags: []
aliases:
  - minimi quadrati
  - residuo
  - equazioni normali
  - SVD
data: "`2024-10-07 14:03`"
---
- # Problema:
	- $Ax=b$ con A di tipo $m \times n$ con $m>n$ 
		- $x\in \mathbb{R}^{n}$, $b\in \mathbb{R}^{m}$ 
	- questo sistema non ammette soluzioni in quanto il problema ha troppe equazioni rispetto alle incognite. 
- # Soluzione:
	- devo trovare una $x$ tale che:
		- $\min_{x\in \mathbb{R}^{n}}||Ax-b||^{2}_2$=F
		- $Ax-b$ è detta _residuo_ $r$  
	- _quindi tra tutte le x di $\mathbb{R}^{n}$ quella che rende il residuo $r= Ax-b$  minimo. _
		- $F:\mathbb{R}^{n} \to \mathbb{R^{+}}$ 
- # Proposizione:
	- Sia $A$ una matrice $m \times n$ con $m>n$ e $rk(A)=k$ allora il problema dei minimi quadrati: $$\min_{x\in \mathbb{R}^{n}}||Ax-b||^{2}_2$$
		- __ammette sempre una soluzione__ ed inoltre:
			- se $k=n$ allora la soluzione è unica
			- se $k<n$ allora ci sono infinite soluzioni e tra queste c'è una $x'$ che ha norma minima.
- # Procedimento:
	- ## Equazioni normali: ^51204a
		- _Normali perché guardano la norma_.
		- minimizzare $||Ax-b||^{2}_2$ quindi:
		- $$(Ax-b)^{T}(Ax-b)=$$
		- $$=(-b^{T}+x^{T}A^{T})(Ax-b)=$$
		- $$=-b^{T}Ax+b^{T}b+x^{T}A^{T}Ax-x^{T}A^{T}b=$$
		- $$=x^{T}A^{T}Ax-2x^{T}Ab+b^{T}b= f(x)$$
		- Per minimizzare la norma devo imporre che la derivata prima sia nulla e visto che le incognite sono più di 1 devo fare il gradiente:
			- $\nabla (x^{T}A^{T}Ax)=2A^{T}Ax$
			- $\nabla(2x^{T}Ab)=2A^{T}b$
			- $\nabla(b^{T}b)=0$ 
		- risulta quindi : 
			- $\nabla f(x)=2A^{T}Ax-2A^{T}b$ e devo trovare il punto in cui questo gradiente è uguale a 0.
			- e posso riscrivere $A^{T}Ax = A^{T}b\in \mathbb{R}^{n}$ 
		- $A^{T}Ax = A^{T}b$ è quindi un sistema lineare con matrice simmetrica e [[Risoluzione di un sistema lineare#^bc9cac|definita positiva]] e quindi si può usare [[Risoluzione di un sistema lineare#^c7c2d7||cholesky]]
		- riducendosi quindi a risolvere il sistema:
			- $$\begin{cases} Ly = A^{T}b \\ L^{T}x=y\end{cases}$$
	- ## Decomposizione in valori singolari (SVD):  ^3e0cd7
		- se $rk(A)<min(m,n)$ ci sono infinite soluzioni e tra tutte queste c'è una $x'=min\| y\|_{2}^{2}$ che appunto ha norma minima e per calcolarla si usa la _singular value decomposition_  
		- $$||Ax-b||^{2}_{2}=||U^{T}Ax-U^{T}b||^{2}_2=$$ 
			- fattibile grazie alle proprietà delle [[Matrice ortogonale||matrici ortogonali]] 
		- $$=||U^{T}AVV^{T}x-U^{T}b||^{2}_2$$
			- risulta anche che $U^{T}AV=\Sigma$ [[autovalori e autovettori#^30ec44||matrice sigma]] 
			- ponendo anche $y=V^{T}x$ e $U^{T}b=g$ risulta infine che:
				- $$||Ax-b||^{2}_{2}=\|\Sigma y-g\|_{2}^{2}$$   
			- quindi minimizzare la funzione iniziale significa minimizzare
				- $$\sum\limits_{i=1}^{k}(\sigma_{i}y_{i}-g_{i})^{2}$$
				- che in particolare vuol dire renderlo nullo, ovvero:
					- $$y_{i}=\frac{g_{i}}{\sigma_{i}}=\frac{U^{T}b}{\sigma_{i}}$$
	- oppure si potrebbe usare la [[Pseudoinversa]] per rappresentare la soluzione del problema in modo più elegante 
- # Link Utili:
	- 